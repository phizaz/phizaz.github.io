<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Tensorflow Summary API V2 | Konpat’s Record of Struggles</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Tensorflow Summary API V2" />
<meta name="author" content="Konpat Preechakul" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Following the announcement in Tensorflow Dev Summit 2018 in Eager Execution part: https://www.youtube.com/watch?v=T8AW0fKP0Hs&amp;vl=en" />
<meta property="og:description" content="Following the announcement in Tensorflow Dev Summit 2018 in Eager Execution part: https://www.youtube.com/watch?v=T8AW0fKP0Hs&amp;vl=en" />
<link rel="canonical" href="https://blog.konpat.me/dev/2018/07/08/tensorflow-summary-api-v2.html" />
<meta property="og:url" content="https://blog.konpat.me/dev/2018/07/08/tensorflow-summary-api-v2.html" />
<meta property="og:site_name" content="Konpat’s Record of Struggles" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-07-08T00:00:00+07:00" />
<script type="application/ld+json">
{"datePublished":"2018-07-08T00:00:00+07:00","description":"Following the announcement in Tensorflow Dev Summit 2018 in Eager Execution part: https://www.youtube.com/watch?v=T8AW0fKP0Hs&amp;vl=en","author":{"@type":"Person","name":"Konpat Preechakul"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.konpat.me/dev/2018/07/08/tensorflow-summary-api-v2.html"},"@type":"BlogPosting","url":"https://blog.konpat.me/dev/2018/07/08/tensorflow-summary-api-v2.html","headline":"Tensorflow Summary API V2","dateModified":"2018-07-08T00:00:00+07:00","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://blog.konpat.me/feed.xml" title="Konpat's Record of Struggles" /><script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "all"
          },
          extensions: ["cancel.js"]
        },
      })
  </script>
  <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML' async></script>

</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Konpat&#39;s Record of Struggles</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>
        
        <div class="trigger">
          <!--
            my_page.autogen is populated by the pagination logic for all pages
                            that are automatically created by the gem. Check for non-existence to exclude pagination pages from site.pages iterators
          -->
          
            
          
            
            <a class="page-link" href="/about/">About</a>
            
          
            
          
            
            <a class="page-link" href="/tags/">Tags</a>
            
          
            
          
            
          
            
            <a class="page-link" href="/academic/index.html">Academic</a>
            
          
            
            <a class="page-link" href="/dev/index.html">Dev</a>
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
            <a class="page-link" href="/me/index.html">Personal</a>
            
          
        </div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Tensorflow Summary API V2</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-07-08T00:00:00+07:00" itemprop="datePublished">Jul 8, 2018
      </time>
      <span>
        
        <a href="/tags/#tensorflow"><span class="badge badge-secondary">tensorflow</span></a>
        
      </span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Following the announcement in Tensorflow Dev Summit 2018 in Eager Execution part: <a href="https://www.youtube.com/watch?v=T8AW0fKP0Hs&amp;vl=en">https://www.youtube.com/watch?v=T8AW0fKP0Hs&amp;vl=en</a></p>

<p>It suggests that we should use <code class="highlighter-rouge">tf.contrib.summary</code> . Here is the link to the documentation <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/summary">https://www.tensorflow.org/api_docs/python/tf/contrib/summary</a></p>

<p>To me, it is not a very good documentation and hard to follow. I have here some example codes to guide you to the new summary API.</p>

<p>I will try to cover both modes, eager and graph. I shall stick with the graph mode and then add some notes for the eager mode.</p>

<p>Overall code template will be put at the end of this article.</p>

<h3 id="creating-the-summary-writer">Creating the summary writer</h3>

<p>This is the same with both graph mode and eager mode.</p>

<p>Since the new summary API is very context based, I think it is a good practice to separate graph for each summary writer.</p>

<p>Initializing the summary:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="s">'logs'</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">create_global_step</span><span class="p">()</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">create_file_writer</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">always_record_summaries</span><span class="p">()</span>
</code></pre></div></div>

<p>Now, we have <code class="highlighter-rouge">graph</code> and <code class="highlighter-rouge">writer</code> .</p>

<p>Note: I usually also create a <code class="highlighter-rouge">global_step</code> with the creation of graph. Which is very convenient because the summary writer will automatically utilize it. You don’t really need to keep the <code class="highlighter-rouge">global_step</code> though, you can always get it from <code class="highlighter-rouge">tf.train.get_global_step()</code> given that you properly set the default graph.</p>

<h3 id="preparing-the-dataset-and-creating-the-model">Preparing the dataset and creating the model</h3>

<p>This is not the main topic of this article though, but for the sake of expressiveness I found it useful for explanation.</p>

<p>First we prepare the dataset, I will demonstrate with <code class="highlighter-rouge">tf.data.Dataset</code> here.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="c1"># dataset
</span>    <span class="n">data</span> <span class="o">=</span> <span class="o">...</span> <span class="n">some</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span> <span class="o">...</span>
    <span class="n">data_itr</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">make_initializable_iterator</span><span class="p">()</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data_itr</span><span class="o">.</span><span class="n">get_next</span><span class="p">()</span> <span class="c1"># x is feature, y is label
</span></code></pre></div></div>

<p>Note that in eager mode you don’t need the <code class="highlighter-rouge">make_initializable_iterator()</code>because the dataset is itself iterable. That means in eager you don’t need <code class="highlighter-rouge">get_next()</code> as well.</p>

<h4 id="at-the-time-we-define-model-we-add-something-to-summary">At the time we define model, we add something to summary</h4>

<p>The following code is in graph mode:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
        <span class="c1"># let's say we have "net" as our model
</span>        <span class="n">prediction_op</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">prediction_op</span><span class="p">)</span>
        <span class="n">opt_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
            <span class="n">loss_op</span><span class="p">,</span> 
            <span class="n">global_step</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_global_step</span><span class="p">())</span>
        
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">record_summaries_every_n_global_steps</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'loss'</span><span class="p">,</span> <span class="n">loss_op</span><span class="p">)</span>
            
        <span class="n">summary_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">all_summary_ops</span><span class="p">()</span>
</code></pre></div></div>

<p>You see that we use <code class="highlighter-rouge">tf.contrib.summary.record_summaries_every_n_global_steps</code> (<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/summary/record_summaries_every_n_global_steps">https://www.tensorflow.org/api_docs/python/tf/contrib/summary/record_summaries_every_n_global_steps</a>) to log anything inside the context manager every “n” steps.</p>

<p>In eager you might consider creating a function for running in each iteration like so:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">eager</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
    
    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
    <span class="n">grads_vars</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span>
        <span class="n">grads_vars</span><span class="p">,</span>
        <span class="n">global_step</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_global_step</span><span class="p">()</span>
    <span class="p">)</span>
    
    <span class="c1"># here is how you log every step (n=1)
</span>    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">record_summaries_every_n_global_steps</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'loss'</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">loss</span>
</code></pre></div></div>

<h3 id="at-the-time-of-running">At the time of running</h3>

<p>Here is the code for graph mode:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
            <span class="c1"># initialize the summary
</span>            <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span>
                <span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="c1"># init vars
</span>            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
            <span class="c1"># init iterator
</span>            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">data_itr</span><span class="o">.</span><span class="n">initializer</span><span class="p">)</span>
            <span class="c1"># run 
</span>            <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span>
                        <span class="n">summary_op</span><span class="p">,</span> <span class="n">opt_op</span><span class="p">,</span> <span class="n">loss_op</span>
                    <span class="p">])</span>
                    <span class="o">...</span>
                <span class="k">except</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">OutOfRangeError</span><span class="p">:</span>
                    <span class="k">break</span>
</code></pre></div></div>

<p>Two things to keep in mind:</p>

<ul>
  <li><code class="highlighter-rouge">tf.contrib.summary.initialize(graph=...)</code> — You can supply “None” to the graph (leave it blank), it won’t save the graph to the Tensorboard. However, this option won’t have any effect in the eager mode anyway (since it doesn’t really construct a graph)</li>
  <li>You need to run <code class="highlighter-rouge">tf.contrib.summary.all_summary_ops()</code> which will actually write the summary. However, you don’t need this for eager mode since it’s executed in real time anyway</li>
</ul>

<h4 id="to-summarize">To summarize</h4>

<ul>
  <li>Create a summary writer under a graph, and use that graph throughout</li>
  <li>Define what you want to include in the summary and how frequent</li>
  <li>Initialize the summary (you might supply the graph here if you want to get the “Graph” page in Tensorboard, won’t work in eager)</li>
  <li>In graph mode, also run <code class="highlighter-rouge">tf.contrib.summary.all_summary_ops()</code> to actually write the summary</li>
</ul>

<h4 id="an-example-of-graph-mode">An example of graph mode</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">save_path</span> <span class="o">=</span> <span class="s">'summary_path'</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">create_global_step</span><span class="p">()</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">create_file_writer</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">always_record_summaries</span><span class="p">()</span>
        
<span class="c1"># simulate dataset
</span><span class="n">fake_dataset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">fake_label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="c1"># preparing a fake dataset
</span><span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">fake_dataset</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">fake_label</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="nb">zip</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>    
    <span class="n">data_itr</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">make_initializable_iterator</span><span class="p">()</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data_itr</span><span class="o">.</span><span class="n">get_next</span><span class="p">()</span>

<span class="c1"># define the computing graph
</span><span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
        <span class="c1"># construct a simple classifier
</span>        <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
        <span class="p">])</span>
        
        <span class="n">prediction_op</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">prediction_op</span><span class="p">)</span>
        <span class="n">opt_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span>
            <span class="n">loss_op</span><span class="p">,</span> 
            <span class="n">global_step</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_global_step</span><span class="p">())</span>
        
        <span class="c1"># here is how you log every step (n=1)
</span>        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">record_summaries_every_n_global_steps</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'loss'</span><span class="p">,</span> <span class="n">loss_op</span><span class="p">)</span>
            
        <span class="n">summary_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">all_summary_ops</span><span class="p">()</span>
        
<span class="c1"># compute the graph
</span><span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
            <span class="c1"># initialize the summary writer
</span>            <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span>
                <span class="n">graph</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="c1"># init vars
</span>            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>
            <span class="c1"># init iterator
</span>            <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">data_itr</span><span class="o">.</span><span class="n">initializer</span><span class="p">)</span>
            <span class="c1"># run until the dataset is exhausted
</span>            <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span>
                        <span class="n">summary_op</span><span class="p">,</span> <span class="n">opt_op</span><span class="p">,</span> <span class="n">loss_op</span>
                    <span class="p">])</span>
                <span class="k">except</span> <span class="n">tf</span><span class="o">.</span><span class="n">errors</span><span class="o">.</span><span class="n">OutOfRangeError</span><span class="p">:</span>
                    <span class="k">break</span>
</code></pre></div></div>

<h4 id="an-example-of-eager-mode">An example of eager mode</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">tf</span><span class="o">.</span><span class="n">enable_eager_execution</span><span class="p">()</span>

<span class="n">save_path</span> <span class="o">=</span> <span class="s">'logs/test10'</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">global_step</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">create_global_step</span><span class="p">()</span>
    <span class="n">writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">create_file_writer</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">always_record_summaries</span><span class="p">()</span>
        
<span class="c1"># simulate dataset
</span><span class="n">fake_dataset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">fake_label</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="c1"># preparing a fake dataset
</span><span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">fake_dataset</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">(</span><span class="n">fake_label</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="nb">zip</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>    

<span class="c1"># define the model
</span><span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
        <span class="c1"># construct a simple classifier
</span>        <span class="n">net</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">),</span>
            <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
        <span class="p">])</span>
        
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>
        
<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">eager</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">prediction</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">sparse_softmax_cross_entropy</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
    
    <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
    <span class="n">grads_vars</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">net</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span>
        <span class="n">grads_vars</span><span class="p">,</span>
        <span class="n">global_step</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">get_global_step</span><span class="p">()</span>
    <span class="p">)</span>
    
    <span class="c1"># here is how you log every step (n=1)
</span>    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">record_summaries_every_n_global_steps</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">scalar</span><span class="p">(</span><span class="s">'loss'</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">loss</span>
        
<span class="c1"># start the training process
</span><span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">writer</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
        <span class="c1"># initialize the summary writer
</span>        <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">initialize</span><span class="p">()</span>
        <span class="c1"># run until the dataset is exhausted
</span>        <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
</code></pre></div></div>


  </div><a class="u-url" href="/dev/2018/07/08/tensorflow-summary-api-v2.html" hidden></a>
</article>



      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col one-half">
        <h2 class="footer-heading">Konpat&#39;s Record of Struggles</h2>
        <ul class="contact-list">
          <li class="p-name">Konpat Preechakul</li><li><a class="u-email" href="mailto:the.akita.ta@gmail.com">the.akita.ta@gmail.com</a></li></ul>
      </div>

      <div class="footer-col one-half">
        <p>O thou, I long to rival thy greatest invention, the mind.</p>
      </div>

      <div class="social-links"><ul class="social-media-list"><li><a href="https://www.facebook.com/phizaz" title="phizaz"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#facebook"></use></svg></a></li><li><a href="https://github.com/phizaz" title="phizaz"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a href="https://www.instagram.com/phizaz" title="phizaz"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#instagram"></use></svg></a></li><li><a href="https://twitter.com/phizaz" title="phizaz"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li><li><a href="/feed.xml" title="rss"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#rss"></use></svg></a></li></ul>
</div>
    </div>

  </div>

</footer></body>

</html>
